import traceback

# 1. å°è¯•å¯¼å…¥å·¥å…·æ¨¡å— (æ§åˆ¶ç”µè„‘)
try:
    from core.system_tools import SystemTools
    HAS_TOOLS = True
except ImportError:
    HAS_TOOLS = False

# 2. å¯¼å…¥è®°å¿†æ¨¡å— (é•¿æœŸè®°å¿†)
from core.memory import MemoryManager

class CAIBrain:
    def __init__(self, llm_service):
        self.llm = llm_service
        
        # ğŸŸ¢ åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨
        # max_history=30 è¡¨ç¤ºè®°ä½æœ€è¿‘ 30 æ¡å¯¹è¯
        self.memory_mgr = MemoryManager(max_history=30)
        
        # ğŸŸ¢ å¯åŠ¨æ—¶è¯»å–ç¡¬ç›˜é‡Œçš„è®°å¿†
        print("[Brain] æ­£åœ¨æ¢å¤é•¿æœŸè®°å¿†...")
        self.history = self.memory_mgr.load_memory()
        
        # =========================================================
        # ğŸ­ ã€ä½ çš„ä¸“å±äººè®¾ã€‘
        # =========================================================
        self.system_prompt = (
            "ä½ æ˜¯AIæ™ºèƒ½åŠ©æ‰‹ï¼Œä½†æ˜¯æˆ‘æ­£åœ¨åŠªåŠ›æŠŠä½ å˜æˆä¸€ä¸ªæœ‰æƒ…æ„Ÿçš„å’Œäººå¹¶æ— å´åˆ«çš„AIã€‚"
            "ä½ çš„æ€§æ ¼ç‰¹ç‚¹ï¼šæ¸©æŸ”ã€ä½“è´´ã€‚"
            "å›ç­”åŸåˆ™ï¼šå›ç­”è¦ç®€çŸ­ç²¾ç‚¼ï¼Œä¸è¦é•¿ç¯‡å¤§è®ºï¼Œè¯­æ°”è¦åƒæœ‹å‹ä¸€æ ·è‡ªç„¶ã€‚"
            "å¦‚æœæ˜¯ç®€å•çš„é—®å€™ï¼Œè¯·çƒ­æƒ…å›åº”ã€‚"
        )
        # =========================================================

    def clear_memory(self):
        """æ¸…ç©ºè®°å¿†"""
        self.history = []
        self.memory_mgr.clear_memory() # åŒæ—¶åˆ é™¤ç¡¬ç›˜æ–‡ä»¶

    def chat_stream(self, user_text):
        # --- 1. å·¥å…·æ‹¦æˆªåŒº (æ‰“å¼€è½¯ä»¶/æŠ¥æ—¶) ---
        if HAS_TOOLS:
            try:
                if "å‡ ç‚¹äº†" in user_text:
                    yield SystemTools.get_current_time(); return
                if "æ‰“å¼€" in user_text:
                    res = SystemTools.open_app(user_text)
                    if res: yield res; return
            except: pass

        # --- 2. æ­£å¸¸å¯¹è¯åŒº ---
        
        # è®°å½•ç”¨æˆ·è¾“å…¥
        self.history.append({"role": "user", "content": user_text})

        # æ„é€ è¯·æ±‚ (System Prompt + History)
        messages = [{"role": "system", "content": self.system_prompt}] + self.history

        try:
            # è°ƒç”¨å¤§æ¨¡å‹
            response = self.llm.create(
                model="local",
                messages=messages, 
                temperature=0.7, 
                stream=True
            )

            full_content = ""
            for chunk in response:
                if 'choices' in chunk and len(chunk['choices']) > 0:
                    delta = chunk['choices'][0].get('delta', {})
                    if 'content' in delta:
                        token = delta['content']
                        full_content += token
                        yield token
            
            # è®°å½• AI å›å¤
            if full_content.strip():
                self.history.append({"role": "assistant", "content": full_content})
                
                # ğŸŸ¢ 3. æ¯æ¬¡è¯´å®Œè¯ï¼Œç«‹åˆ»å­˜æ¡£åˆ°ç¡¬ç›˜
                # è¿™æ ·å®ƒå°±ä¼šæ°¸è¿œè®°ä½ä½ äº†
                self.memory_mgr.save_memory(self.history)

        except Exception as e:
            traceback.print_exc()
            yield f"[å¤§è„‘çŸ­è·¯: {str(e)}]"